{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This purpose of this notebook is to create the final datasets used in my final project for the \"Open Data Mashups\" class.   \n",
    "\n",
    "This notebook is organized into three phases that are used to create the following three datasets: PR_dataset, ChiHealthAtlas_dataset, and ChiNeighborhoodsMapping_dataset. \n",
    "\n",
    "First, we will create the PR_dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PHASE 1 - Create the PR_dataset\n",
    "\n",
    "Step 1: Obtain the 2017 URL links. \n",
    "\n",
    "For this step, I had help from Elizabeth to obtain these URLs. They are formatted so that each URL is on its own line. These URLs are available in the .txt document titled \"2017urls.txt\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Webscrape the data from the Chicago's Mayor Office's website. \n",
    "\n",
    "The script below uses the \"2017urls.txt\" file to hit all of the 2017 press releases published by the Chicago's Mayor Office. The script gathers the HTML source code from each press release web page and saves it as a .txt file that is named and saved using incrementing numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '2017urls.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-44bb3c7b3b94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#Opens list of 2017 URLs and reads them into a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0minfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2017urls.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0murl_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '2017urls.txt'"
     ]
    }
   ],
   "source": [
    "#This script takes a list of URLs of 2017 Mayor press releases as an input and loops through this list.\n",
    "#This loop gets the HTML source code for each of those pages that is associated with those URLs and saves them as an individual .txt file\n",
    "import urllib.request\n",
    "import time\n",
    "\n",
    "#Opens list of 2017 URLs and reads them into a list\n",
    "infile = open(\"2017urls.txt\", \"r\")\n",
    "url_list = infile.readlines()\n",
    "\n",
    "#Loops through URLS, fetching the HTML for each and writing to a .txt file which is numbered and written incrementally\n",
    "i=0\n",
    "for each_url in url_list:\n",
    "    urllib.request.urlretrieve(each_url, \"pr\" + str(i) + \".txt\")\n",
    "    i = i + 1\n",
    "    time.sleep(3)\n",
    "    print(\"Sleeping for 3 seconds....\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: \n",
    "\n",
    "Use the output files from Step 2 to create a CSV with only the relevant press release data. \n",
    "\n",
    "The script below will iterate through the output files from Step 2 to parse data about the press releases from the HTML source code. This script uses BeautifulSoup to parse the press release title, publication date, and body text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pr0.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-09000da1849c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1003\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0minfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pr\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mpage_html\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pr0.txt'"
     ]
    }
   ],
   "source": [
    "#This updated version of my script extracts the necessary data from the press releases and writes it to a CSV file.\n",
    "#This update version uses .txt files saved to a local drive instead of directly hitting websites and then extracting.\n",
    "\n",
    "\n",
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup\n",
    "\n",
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "\n",
    "#from time import sleep\n",
    "#from random import randint\n",
    "\n",
    "\n",
    "\n",
    "# names csv file, gives it headers\n",
    "filename = \"opendata3_testfortextfunction.csv\"\n",
    "f = open(filename, \"w\", encoding=\"utf-8\")\n",
    "headers = \"title, body, date_published\\n\"\n",
    "f.write(headers)\n",
    "\n",
    "#Starts a loop to loop through names of files saved on local drive, turns each file into soup object, parses those files\n",
    "\n",
    "for files in range(0,1003):\n",
    "    infile = open(\"pr\" + str(files) + \".txt\", \"rb\")\n",
    "    page_html = infile.read()\n",
    "\n",
    "\n",
    "\n",
    "    page_soup = soup(page_html, \"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Finds title, subtitle, date, and body elements\n",
    "    title = page_soup.find(\"h1\",{\"class\":\"bottom-spacing page-heading\"}).contents[0]\n",
    "\n",
    "    #this will need an if else statement to capture the pr without subtitles\n",
    "    #subtitles = []\n",
    "    #for subtitle in page_soup.find(\"div\", {\"class\": \"bottom-spacing black-normal-title\"}).contents[0]:\n",
    "    #    subtitles.append(subtitle.text)\n",
    "    #    if subtitle is None:\n",
    "    #        print(\"None\")\n",
    "\n",
    "    body_of_pr = page_soup.find(\"div\",{\"class\":\"container-fluid page-full-description\"}).find('p').text\n",
    "    str_body_of_pr = str(body_of_pr)\n",
    "\n",
    "    date_published = page_soup.find(\"div\",{\"class\":\"black-medium-high-title\"}).contents[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Writes these elements to a csv file and replaces all commas with other characters, because csv are comma delimited.\n",
    "    f.write(title.replace(\",\", \"|\") + \",\" + str_body_of_pr.replace(\",\", \"|\") + \",\" + date_published.replace(\",\", \"|\") + \"\\n\")\n",
    "\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Clean the csv output file from Step 2. \n",
    "\n",
    "The output file from Step 2 will have some issues that need to be cleaned. For instance all of the commas will are replaced. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert code here that will help me clean all of the issues with the opendata3_testfortextfunction.csv final dataset file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Begin data analysis using a textual analysis library! \n",
    "\n",
    "For this project, the csv dataset that is outputted from Steps 1-3 is utilized to perform textual analysis. \n",
    "The script below creates a column of data that represents this analysis. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a script that uses Spacy to create a column that outputs all of the GEO data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Begin data analysis that uses the find.all() command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a script where you are just looping through the body of the press release trying to find exact phrasing of \n",
    "#a neighborhoods name. I'm HAVING TROUBLEEEEEE! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GOOD JOB! You have created ONE OF your FINAL datasets! BRAVO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PHASE 2 - Create the ChiHealthAtlas_dataset.csv\n",
    "\n",
    "The next phase in this project is to create the files for the second dataset, ChiHealthAtlas_dataset. This requires hand cleaning which I will describe here for the original three ChiHealthAtlas data files. These three original files are called: \"Median_household_income_ChiHealthAtlas.xlsx\", \"Unemployment_ChiHealtAtlas.xlsx\", and \"Violent_crime_in_public_spaces.xlsx.\" \n",
    "\n",
    "\"Median_household_income_ChiHealthAtlas.xlsx\"\n",
    "\n",
    "-Delete columns A, B, C, K-AA. \n",
    "-Delete rows 32-41, 119 - 272\n",
    "\n",
    "\"Unemployment_ChiHealtAtlas.xlsx\"\n",
    "\n",
    "-Delete columns A, B, K-R, T-AA\n",
    "-Delete rows 44-57, 135-288\n",
    "\n",
    "\"Violent_crime_in_public_spaces.xlsx\"\n",
    "\n",
    "-Delete columns A, B, K-AA\n",
    "-Delete rows 2-174\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PHASE 3 - Clean the ChiNeighborhoods_dataset.csv\n",
    "\n",
    "This dataset was downloaded from Tableau Public and created by another author. This data will be used to map the PR_dataset to the Chicago neighborhood they are about. This dataset is added in order to be able to visualize this data using a data visualization, mapping tool. \n",
    "\n",
    "WHAT TYPE OF CLEANING DO I NEED HERE TO BE ABLE TO ACCOMPLISH THIS? \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This purpose of this notebook is to create the final datasets used in my final project for the \"Open Data Mashups\" class.   \n",
    "\n",
    "This notebook is organized into three phases that are used to create the following three datasets: PR_dataset, ChiHealthAtlas_dataset, and ChiNeighborhoodsMapping_dataset. \n",
    "\n",
    "First, we will create the PR_dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PHASE 1 - Create the PR_dataset\n",
    "\n",
    "Step 1: Obtain the 2017 URL links. \n",
    "\n",
    "For this step, I had help from Elizabeth to obtain these URLs. They are formatted so that each URL is on its own line. These URLs are available in the .txt document titled \"2017urls.txt\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Webscrape the data from the Chicago's Mayor Office's website. \n",
    "\n",
    "The script below uses the \"2017urls.txt\" file to hit all of the 2017 press releases published by the Chicago's Mayor Office. The script gathers the HTML source code from each press release web page and saves it as a .txt file that is named and saved using incrementing numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '2017urls.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-44bb3c7b3b94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#Opens list of 2017 URLs and reads them into a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0minfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2017urls.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0murl_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '2017urls.txt'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "n_data = {} # This creates a dictionary object\n",
    "\n",
    "\n",
    "#Here, I open my press release dataset (w. unique identifiers), read it in, move past problematic headers, and (put all of them in a list?)\n",
    "f = open('Intermediate_PR_dataset.csv')\n",
    "pr_text = csv.reader(f, delimiter=',', )\n",
    "next(pr_text) # move past headers\n",
    "next(pr_text) #move past headers\n",
    "pr_list = [r for r in pr_text] # I have no idea what this line does. Does this make a list of each row in my dataset?\n",
    "\n",
    "f2 = open('Unemployment_ChiHealthAtlas.csv')\n",
    "unemployment_data = csv.reader(f2, delimiter=',', )\n",
    "unemployment_list = [r for r in unemployment_data]\n",
    "\n",
    "\n",
    "f3 = open('Violent_crime_in_public_spaces_ChiHealthAtlas.csv')\n",
    "crime_data = csv.reader(f3, delimiter=',', )\n",
    "crime_list = [r for r in crime_data]\n",
    "\n",
    "\n",
    "f4 = open('Medianincome_ChiHealthAtlas.csv')\n",
    "median_income_data = csv.reader(f4, delimiter=',', )\n",
    "median_income_list = [r for r in median_income_data]\n",
    "\n",
    "\n",
    "\n",
    "# for row in pr_text:\n",
    "#     title = row[0]\n",
    "#     text = row[1]\n",
    "#     date = row[2]\n",
    "#     pr_list.append(text)\n",
    "\n",
    "#Here I'm opening a file with all the neighborhood names and their alternate names, read it in, create a list of lists\n",
    "with open('altnames_csv.csv', 'r') as infile:\n",
    "    csvin = csv.reader(infile)\n",
    "    data = [r for r in csvin] #this is a list of lists\n",
    "\n",
    "for nhood in data:\n",
    "    std_name = nhood[0] #For each list in my list of lists of altnames, grab the first value\n",
    "    n_data[std_name] = {\"names\" : nhood, \"appears\": 0, \"pr_id\": []} #Set the standard neighborhood name as the key and the alt names as the value of names, fills other values with empty data\n",
    "\n",
    "for pr in pr_list:\n",
    "    pr_id = pr[0]\n",
    "    title = pr[1]\n",
    "    text = pr[2]\n",
    "    date = pr[3]\n",
    "    for nid in n_data.keys(): #Looks up all the std names\n",
    "        allnames = n_data[nid][\"names\"] #Gives a list of the names\n",
    "        for name in allnames:\n",
    "            if name in text:\n",
    "                n_data[nid][\"pr_id\"].append(pr_id)\n",
    "                n_data[nid][\"appears\"] += 1\n",
    "                break\n",
    "\n",
    "for column in unemployment_list:\n",
    "    std_name = column[0]\n",
    "    geo_id = column[1]\n",
    "    percent = column[2]\n",
    "    if \"-\" in std_name:\n",
    "        std_name = std_name.split(\"-\")[-1]\n",
    "    n_data[std_name][\"geo_id\"]= geo_id\n",
    "    n_data[std_name][\"percent\"]= percent\n",
    "\n",
    "\n",
    "for item in crime_list:\n",
    "    year = item[0]\n",
    "    std_name = item[1]\n",
    "    crime = item[3]\n",
    "    if \"-\" in std_name:\n",
    "        std_name = std_name.split(\"-\")[-1]\n",
    "    n_data[std_name][\"crime\"]= crime\n",
    "\n",
    "for item4 in median_income_list:\n",
    "    std_name = item4[0]\n",
    "    geo_id = item4[1]\n",
    "    median_income = item4[2]\n",
    "    if \"-\" in std_name:\n",
    "        std_name = std_name.split(\"-\")[-1]\n",
    "    n_data[std_name][\"median income\"]= median_income\n",
    "    n_data[std_name][\"geo_id\"]= geo_id\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for nid, vals in n_data.items():\n",
    "    print(nid, vals)\n",
    "    # print(nid, vals['appears'], vals['pr_id'], vals['unemployment'])\n",
    "\n",
    "with open('nhooddata.json', 'w') as fout:\n",
    "    json.dump(n_data, fout, indent = 4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Aileen attempting to write this JSON into a CSV\n",
    "#n_data_parsed = json.loads()\n",
    "\n",
    "\n",
    "outputfile = open('Final_dataset.csv', 'w')\n",
    "csvwriter = csv.writer(outputfile)\n",
    "\n",
    "columnTitleRow = \"std_name, geo_id, percent_unemployment, crime, median_income, appears, pr_id \\n\"\n",
    "outputfile.write(columnTitleRow)\n",
    "\n",
    "for key in n_data.keys():\n",
    "    std_name = key\n",
    "    geo_id = str(n_data[key][\"geo_id\"])\n",
    "    percent = str(n_data[key][\"percent\"])\n",
    "    crime = str(n_data[key][\"crime\"])\n",
    "    median_income = str(n_data[key][\"median income\"])\n",
    "    appears = str(n_data[key][\"appears\"])\n",
    "    pr_id = '\"' + str(n_data[key][\"pr_id\"]) + '\"'\n",
    "    row = std_name + \",\" + geo_id + \",\" + percent + \",\" + crime + \",\" + median_income + \",\" + appears + \",\" + pr_id + \",\" + \"\\n\"\n",
    "    outputfile.write(row)\n",
    "\n",
    "outputfile.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: \n",
    "\n",
    "Use the output files from Step 2 to create a CSV with only the relevant press release data. \n",
    "\n",
    "The script below will iterate through the output files from Step 2 to parse data about the press releases from the HTML source code. This script uses BeautifulSoup to parse the press release title, publication date, and body text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pr0.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-09000da1849c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1003\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0minfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pr\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mpage_html\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pr0.txt'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Build a dictionary of relevant data values that searches within the body of press release to identify whether a neighborhood is mentioned. Output that dictionary as a JSON file.\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert code here that will help me clean all of the issues with the opendata3_testfortextfunction.csv final dataset file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4:  Use the JSON file in combination with Chicago community area spatial files to visualize the data. \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a script that uses Spacy to create a column that outputs all of the GEO data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GOOD JOB! You have created ONE OF your FINAL datasets! BRAVO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PHASE 2 - Create the ChiHealthAtlas_dataset.csv\n",
    "\n",
    "The next phase in this project is to create the files for the second dataset, ChiHealthAtlas_dataset. This requires hand cleaning which I will describe here for the original three ChiHealthAtlas data files. These three original files are called: \"Median_household_income_ChiHealthAtlas.xlsx\", \"Unemployment_ChiHealtAtlas.xlsx\", and \"Violent_crime_in_public_spaces.xlsx.\" \n",
    "\n",
    "\"Median_household_income_ChiHealthAtlas.xlsx\"\n",
    "\n",
    "-Delete columns A, B, C, K-AA. \n",
    "-Delete rows 32-41, 119 - 272\n",
    "\n",
    "\"Unemployment_ChiHealtAtlas.xlsx\"\n",
    "\n",
    "-Delete columns A, B, K-R, T-AA\n",
    "-Delete rows 44-57, 135-288\n",
    "\n",
    "\"Violent_crime_in_public_spaces.xlsx\"\n",
    "\n",
    "-Delete columns A, B, K-AA\n",
    "-Delete rows 2-174\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PHASE 3 - Clean the ChiNeighborhoods_dataset.csv\n",
    "\n",
    "This dataset was downloaded from Tableau Public and created by another author. This data will be used to map the PR_dataset to the Chicago neighborhood they are about. This dataset is added in order to be able to visualize this data using a data visualization, mapping tool. \n",
    "\n",
    "WHAT TYPE OF CLEANING DO I NEED HERE TO BE ABLE TO ACCOMPLISH THIS? \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
